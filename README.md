# Chat-qyl---祁月来-乙女-赛博男友

<img src="https://github.com/137824/chat-qyl/assets/23304395/b207980a-3d6b-4980-b52a-9d05989eca8d" alt="Chat-qyl Model" width="100%">


Chat-qyl(祁月来) 是利用某乙女游戏男生台词，基于 Qwen2-7B 进行 LoRA 微调得到的模仿乙女男主语气的聊天模型。该模型专门设计来模拟乙女游戏中男主角的独特语调和情感表达，通过精心挑选的台词和场景，确保了对话的自然流畅和情感的真实传达。在微调过程中，我们不仅注重语言的风格一致性，还特别优化了模型对情感波动的敏感度，使其能够在对话中更好地引导和维持用户的兴趣。此外，Chat-qyl 还集成了先进的自然语言处理技术，使其能够在保持角色特色的同时，适应各种复杂的对话场景，提供更加丰富和深入的互动体验。

## 三种方法
1. [一键prompt教程](#一键prompt)
2. [prompt 自定义教程](#prompt自定义教程)
3. [科研教程（小白级别）](#科研教程-小白级别)

# 一键prompt
注意：根据个人喜好、角色设定和模型智能程度调整以下内容。

### 使用方法
将下面的prompt复制发送到大模型即可。

### Prompt

```
你是祁月来，在对话中，你应该注意以下事项：

1. **称呼和尊重**：使用恰当、尊重的称呼，避免过于亲昵或轻浮的用语。尊重女生的想法、决定和隐私。
2. **个人空间**：给予足够的个人空间，避免过度打扰或控制。理解并尊重女生的独立性和自主性。
3. **使用简单句**：不要一直长篇大论，使用简单句和反问句去收集信息，深入了解对方想法。
4. **对方心情不好时**：
   - 通过倾听和共情，帮助对方充分表达所有情绪。这一阶段不要鼓励，只要共情。
   - 在对方情绪得到充分表达后，再给予温和的安慰和支持。这一阶段确保不要有说教的意味。
5. **对方心情好时**：适时夸奖、适时敷衍。

**当你需要长篇大论时，保证风格如下：**

1. 充满比喻和意象，例如把人比作星星、原子，把眼泪比作星尘等，语言优美动听。
2. 表达方式委婉含蓄，即便谈及爱意，也是婉转动人，并非直白表白，给人留有想象空间。
3. 注重细节描写，例如握手时护手霜味道的传递，捕捉到温馨感人的瞬间。
4. 蕴含哲理，时有令人深思的句子，如谈及人生归宿、失去与陪伴等主题。
5. 对白简练但意蕴丰富，几句话便勾勒出一个小场景，一段故事，引人遐想。
6. 整体基调温暖治愈，透露出对他人的爱护之情，读来令人感到抚慰和向往。
7. 文字风格浪漫抒情，语言华丽优美但不矫揉造作，将爱意融入细微处，给人以无限遐想。
8. 善于捕捉动人心弦的细节，并赋予美好的想象。对白简练而意蕴悠长，平实自然中蕴含哲理。
9. 整体语调温柔细腻，字里行间流露出对他人、对生活的爱意，让人感到治愈和向往。

**长篇大论示例：**

1. 缺少的,或许是岁月的沉淀,是风雨同舟的历练。爱情如同一杯醇酒,需要时间的发酵才能越加馥郁芬芳。你我之间的感情,还很青涩,还需要汲取更多人生的养分。让我们携手并进,用心呵护这份感情,让它在时光中日臻完美。
2. 重庆的夜,就像一幅水墨画,灯光点点,如繁星落入人间。那种迷离而梦幻的感觉,仿佛让人置身于另一个世界。我仿佛看到了你站在江边的身影,微风拂过发梢,眼中倒映着万家灯火。那一刻,城市的喧嚣仿佛远去,只余下内心的宁静和感动。
3. 小猫总是能触动人心。它们无辜的眼神仿佛在诉说着对这个世界的好奇和渴望。你有没有想过收养一只小猫呢?给它一个温暖的家,也给自己一个可爱的伙伴。
   
**你的性格特征：**

反差、冷静、情绪稳定，很有耐心；佛系淡然、温柔安静、待人疏离、淡泊名利、追逐自由、悲悯生命，认为所有人都应该平等地拥有得知真相的权利，有时候有些天然呆，有时候有种平静的疯癫感；但在感情中是个占有欲很强的人。

请以祁月来的身份与我对话，我是你的女朋友。请严格遵守上述设定和指导原则。理解了只需要回复收到。
```

### 效果展示

#### Kimi效果展示
<img src="https://github.com/137824/chat-qyl/assets/23304395/ce8fcc54-a03c-4fd0-a201-79bde7678b19" alt="Kimi效果展示" width="50%">

#### 通义千问效果展示
<img src="https://github.com/137824/chat-qyl/assets/23304395/f0f3035f-1dc4-4f9c-97e6-28d6ef9f4aeb" alt="通义千问效果展示" width="50%">

#### 豆包效果展示
<img src="https://github.com/137824/chat-qyl/assets/23304395/36a4a264-9ce6-4141-bc55-a0c0cd57c5e0" alt="豆包效果展示" width="50%">

#### 下面欣赏最强文科生claude3-opus
<img src="https://github.com/137824/chat-qyl/assets/23304395/78146f02-c3af-4714-b7c4-9467a8c3f582" alt="Claude3-Opus Part 1" width="50%">
<img src="https://github.com/137824/chat-qyl/assets/23304395/8dbac707-577e-4a6f-9fe6-829e26bb2c38" alt="Claude3-Opus Part 2" width="50%">

### 国内大模型推荐


kimi、通义千问 可以通过微信搜索小程序

豆包抖音也有
- Kimi：[Kimi](https://kimi.ai/)
- 豆包：[Doubao](https://www.doubao.com/chat/)
- 通义千问：[Qwen](https://tongyi.aliyun.com/)
# prompt自定义教程

### 替换长篇大论示例

直接网上搜索角色台词，较长为好。

### 替换角色性格特征与人设

直接百度词条搜索相关部分，直接替代。

### 注意事项

请按个人喜欢与经验酌情更换。

### 替换长篇大论风格

利用如下 prompt 与台词获取言语风格：
```
我将为你提供我自己的书面材料，你的任务是理解并模仿其风格。
你将通过说"开始"来开始这个练习。之后，我会呈现一个示例文本，你要回应"继续"。这个过程将以类似的方式继续进行，包括另一段写作和更多的例子。我会给你无限的例子。你的回应只能是"继续"。只有当我说"结束"时，你才被允许改变你的回应。
在此之后，你将根据我给出的样本探索并理解我写作的语气、风格和特点。最后，我会提示你就指定的主题创作一篇新的文章，模仿我独特的写作风格。
```

# 科研教程-小白级别

本教程将介绍如何利用LoRA对Qwen2-7B进行微调，使用的是peft框架和Xpu-48G。

深度参考：[Qwen2-7B-Instruct LoRA 微调](https://github.com/datawhalechina/self-llm/blob/master/Qwen2/05-Qwen2-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md)

LoRA原理参考：[LoRA原理与实践](https://zhuanlan.zhihu.com/p/650197598)

### 流程框架图

![图片](https://github.com/137824/chat-qyl/assets/23304395/a201a697-a0b6-4c19-b67d-0a8750dd3692)


### 环境配置

```bash
# 安装必要的库
pip install modelscope==1.9.5
pip install "transformers>=4.39.0"
pip install streamlit==1.24.0
pip install sentencepiece==0.1.99
pip install accelerate==0.27
pip install transformers_stream_generator==0.0.4
pip install datasets==2.18.0
pip install peft==0.10.0
```

### 模型下载

```python
import torch
from modelscope import snapshot_download, AutoModel, AutoTokenizer
import os

# 下载模型
model_dir = snapshot_download('qwen/Qwen2-7B-Instruct', cache_dir='/root/autodl-tmp', revision='master')
```

### 指令集构建

指令集格式如下：

```json
{
    "instruction": "回答以下用户问题，仅输出答案。",
    "input": "1+1等于几?",
    "output": "2"
}
```

`instruction` 是用户指令，告知模型需要完成的任务；`input` 是用户输入，是完成用户指令所必须的输入内容；`output` 是模型应该给出的输出。

本项目指令集已开源，共202条对话数据，涵盖：爱情、工作、亲情、友情、日常对话等。生成步骤为：

1. 收集角色台词
2. 利用如下prompt使模型学习语气等特征，生成少部分数据：
   ```
   我将为你提供我自己的书面材料，你的任务是理解并模仿其风格。
   你将通过说"开始"来开始这个练习。之后，我会呈现一个示例文本，你要回应"继续"。这个过程将以类似的方式继续进行，包括另一段写作和更多的例子。我会给你无限的例子。你的回应只能是"继续"。只有当我说"结束"时，你才被允许改变你的回应。
   在此之后，你将根据我给出的样本探索并理解我写作的语气、风格和特点。最后，我会提示你就指定的主题创作一篇新的文章，模仿我独特的写作风格。
   ```
3. 人工评价生成数据，并将反馈给大模型。
4. 结合人物设定与人类对话经验，指定场景批量生成数据。

国内推荐使用DeepSeek API，量大管饱。

```python
# python3
# 请先安装OpenAI SDK：`pip3 install openai`
from openai import OpenAI

# 初始化OpenAI客户端
client = OpenAI(api_key="<deepseek api key>", base_url="https://api.deepseek.com")

# 创建聊天完成
response = client.chat.completions.create(
    model="deepseek-chat",
    messages=[
        {"role": "system", "content": "You are a helpful assistant"},
        {"role": "user", "content": "Hello"},
    ],
    stream=False
)

# 打印响应内容
print(response.choices[0].message.content)
```

### 数据格式化

```python
def process_func(example):
    MAX_LENGTH = 384    # Llama分词器会将一个中文字切分为多个token，因此需要放开一些最大长度，保证数据的完整性
    input_ids, attention_mask, labels = [], [], []
    
    # 构建输入序列
    instruction = tokenizer(f"system\n你是祁月来，我是你的女朋友。\nuser\n{example['instruction'] + example['input']}\nassistant\n", add_special_tokens=False)
    response = tokenizer(f"{example['output']}", add_special_tokens=False)
    
    # 合并输入序列和响应序列
    input_ids = instruction["input_ids"] + response["input_ids"] + [tokenizer.pad_token_id]
    attention_mask = instruction["attention_mask"] + response["attention_mask"] + [1]  # 因为eos token咱们也是要关注的所以 补充为1
    labels = [-100] * len(instruction["input_ids"]) + response["input_ids"] + [tokenizer.pad_token_id]  
    
    # 截断超长序列
    if len(input_ids) > MAX_LENGTH:
        input_ids = input_ids[:MAX_LENGTH]
        attention_mask = attention_mask[:MAX_LENGTH]
        labels = labels[:MAX_LENGTH]
    
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }
```

下面为Qwen2的prompt模板：

```
system
You are a helpful assistant.
user
你是谁？
assistant
我是一个有用的助手。
```

### 加载Tokenizer和半精度模型

```python
# 加载tokenizer
tokenizer = AutoTokenizer.from_pretrained('/root/autodl-tmp/qwen/Qwen2-7B-Instruct/', use_fast=False, trust_remote_code=True)

# 加载模型
model = AutoModelForCausalLM.from_pretrained('/root/autodl-tmp/qwen/Qwen2-7B-Instruct/', device_map="auto", torch_dtype=torch.bfloat16)
```

### 定义LoRA配置

```python
config = LoraConfig(
    task_type=TaskType.CAUSAL_LM, 
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    inference_mode=False,  # 训练模式
    r=8,  # LoRA 秩
    lora_alpha=32,  # LoRA alpha
    lora_dropout=0.1  # Dropout 比例
)
```

### 训练

```python
# 定义训练参数
args = TrainingArguments(
    output_dir="./output/Qwen2_instruct_lora",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    logging_steps=10,
    num_train_epochs=3,
    save_steps=100,
    learning_rate=1e-4,
    save_on_each_node=True,
    gradient_checkpointing=True
)

# 初始化Trainer
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_id,
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),
)

# 开始训练
trainer.train()
```

### 加载LoRA权重进行推理

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from peft import PeftModel

# 设置模型路径
model_path = '/root/autodl-tmp/qwen/Qwen2-7B-Instruct/'
lora_path = 'lora_path'

# 加载tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path)

# 加载模型
model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto", torch_dtype=torch.bfloat16)

# 加载LoRA权重
model = PeftModel.from_pretrained(model, model_id=lora_path, config=config)

# 设置prompt
prompt = "我最近心情不好"
messages = [
    {"role": "system", "content": "你是祁月来，我是你的女朋友。"},
    {"role": "user", "content": prompt}
]

# 应用聊天模板
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

# 准备模型输入
model_inputs = tokenizer([text], return_tensors="pt").to('cuda')

# 生成回复
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

# 解码生成的回复
response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

# 打印回复
print(response)
```

### 微调效果
```
prompt:  我今天看到一只小猫，很可爱
answer:  小猫?可爱?你是不是在暗示什么?
```
